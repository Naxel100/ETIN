main.py:6: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path='config', config_name='config')
/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Global seed set to 69
[2022-09-28 13:01:59,340][torch.distributed.nn.jit.instantiator][INFO] - Created a temporary directory at /state/partition1/slurm_tmp/20141498.4294967291.0/tmpmu51mzfy
[2022-09-28 13:01:59,342][torch.distributed.nn.jit.instantiator][INFO] - Writing /state/partition1/slurm_tmp/20141498.4294967291.0/tmpmu51mzfy/_remote_module_non_sriptable.py
Error executing job with overrides: []
Traceback (most recent call last):
  File "main.py", line 22, in <module>
    main()
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/utils.py", line 216, in run_and_report
    raise ex
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 13, in main
    etin = ETIN(cfg.Language, cfg.Model, seed=69)
  File "/home/gridsan/amorenas/ETIN3/scripts/etin.py", line 46, in __init__
    self.etin_model = ETIN_model.load_from_checkpoint(model_cfg.from_path, cfg=self.model_cfg, info_for_model=self.language.info_for_model)
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 161, in load_from_checkpoint
    model = cls._load_model_state(checkpoint, strict=strict, **kwargs)
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/pytorch_lightning/core/saving.py", line 209, in _load_model_state
    keys = model.load_state_dict(checkpoint["state_dict"], strict=strict)
  File "/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1497, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ETIN_model:
	size mismatch for pos_embedding.weight: copying a param with shape torch.Size([32, 512]) from checkpoint, the shape in current model is torch.Size([16, 512]).
	size mismatch for set_encoder.selfatt1.mab0.fc_k.weight: copying a param with shape torch.Size([512, 80]) from checkpoint, the shape in current model is torch.Size([512, 64]).
	size mismatch for set_encoder.selfatt1.mab0.fc_v.weight: copying a param with shape torch.Size([512, 80]) from checkpoint, the shape in current model is torch.Size([512, 64]).
	size mismatch for set_encoder.selfatt1.mab1.fc_q.weight: copying a param with shape torch.Size([512, 80]) from checkpoint, the shape in current model is torch.Size([512, 64]).
