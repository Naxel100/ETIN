{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "█ █▀▄▀█ █▀█ █▀█ █▀█ ▀█▀ █▀\n",
    "█ █░▀░█ █▀▀ █▄█ █▀▄ ░█░ ▄█\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "█▀▄▀█ ▄▀█ █ █▄░█\n",
    "█░▀░█ █▀█ █ █░▀█\n",
    "'''\n",
    "\n",
    "# Create an Expression Tree Improver Network (ETIN)\n",
    "etin = ETIN()\n",
    "# Train the ETIN\n",
    "etin.train()\n",
    "# ETIN ready to be used. Example:\n",
    "# etin.improve(example_expression_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETIN_model(torch.nn.Module):\n",
    "    '''\n",
    "    This class is a wrapper for the Expression Tree Improver Network (ETIN). It is a neural network that takes as input a \n",
    "    an expression tree, dataset and, the predicted error and, it outputs a new expression tree with a lower general prediction error on the dataset.\n",
    "    The structure of the network is as follows:\n",
    "        - Input: Expression Tree, Dataset, Predicted Error\n",
    "        - 2 Summarizers: 2 Transformer networks dedicated to create an encoded representation of the dataset and the prediction errors.\n",
    "        - 1 Transformer Encoder: Transformer network that takes as input the embedding of each token of the expression tree.\n",
    "        - 1 Linear Layer: Linear layer that takes as input the concatenation of the encoded dataset, encoded prediction error and encoded_embedding.\n",
    "        - Output: Expression Tree with a lower general prediction error on the dataset.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, encoded_size, emb_size, language_size, max_seq_size):\n",
    "        super(ETIN_model, self).__init__()\n",
    "        self.dataset_summarizer = Summarizer()\n",
    "        self.prediction_error_summarizer = Summarizer()\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=max_seq_size, nhead=8)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.output_layer = torch.nn.Linear(in_features=2*encoded_size + emb_size, out_features=language_size)\n",
    "\n",
    "    \n",
    "    def forward(self, expression_tree, dataset, prediction_error):\n",
    "        '''\n",
    "        This function is the forward pass of the ETIN. It takes as input an expression tree, dataset and, the predicted error and, it outputs a new expression tree with a lower general prediction error on the dataset.\n",
    "        '''\n",
    "        # Create encoded dataset  (B x 100 x 2) -> (B x E)\n",
    "        encoded_dataset = self.dataset_summarizer(dataset)   # Posibilidad de meter un diccionario para no tener que recalcular.\n",
    "        # Create encoded prediction error  (B x 100 x 2) -> (B x E)\n",
    "        encoded_prediction_error = self.prediction_error_summarizer(prediction_error)\n",
    "\n",
    "        # Create Embedding from each token of the Expression Tree (B x L x 3M) -> (B x L x G)\n",
    "        embedding = self.create_embedding(expression_tree)\n",
    "        # Pass the embedding through the Transformer Encoder (B x L x G) -> (B x L x G)\n",
    "        embedding = self.transformer_encoder(embedding)\n",
    "\n",
    "        # Concatenate encoded dataset, encoded prediction error and encoded_embedding (B x L x 2E+G) -> (B x L x 2E+G)\n",
    "        concatenated_input = torch.cat((encoded_dataset, encoded_prediction_error, embedding), dim=2)\n",
    "\n",
    "        # Pass the concatenated input through the Linear final layer (B x L x 2E+G) -> (B x L x M)\n",
    "        output = self.output_layer(concatenated_input)\n",
    "\n",
    "        # Return the output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 0])\n",
    "torch.where(a == 1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "class Token():\n",
    "    def __init__(self, function, arity, symbol, unprotected_function, inv=None):\n",
    "        self.function = function\n",
    "        self.arity = arity\n",
    "        self.symbol = symbol\n",
    "        self.inv = inv\n",
    "        self.unprotected_function = unprotected_function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Restrictions():\n",
    "\n",
    "    permitted = {\n",
    "        'no_inverse_parent': lambda parent: Restrictions.no_inverse_parent(parent),\n",
    "        'no_double_exp': lambda parent: Restrictions.no_double_exp(parent),\n",
    "        'no_sqrt_in_log': lambda parent: Restrictions.no_sqrt_in_log(parent),\n",
    "        'no_double_division': lambda parent: Restrictions.no_double_division(parent),\n",
    "        'DistributiveRestriction': lambda not_allowed, parent, n_samples, needed: Restrictions.DistributiveRestriction(not_allowed, parent, n_samples, needed),\n",
    "        'no_const_after_unary': lambda parent: Restrictions.no_const_after_unary(parent),\n",
    "        'no_sqrt_in_sqrt': lambda parent: Restrictions.no_sqrt_in_sqrt(parent),\n",
    "        'no_trigo_in_log': lambda parent: Restrictions.no_trigo_in_log(parent),\n",
    "        'no_trigo_in_exp': lambda parent: Restrictions.no_trigo_in_exp(parent),\n",
    "        'no_trigo_offspring': lambda trigo_offspring: Restrictions.no_trigo_offspring(trigo_offspring),\n",
    "        'no_tan_in_exp': lambda parent: Restrictions.no_tan_in_exp(parent),\n",
    "    }\n",
    "    restrictions=None\n",
    "\n",
    "    def __init__(self, restrictions):\n",
    "        for restriction in restrictions:\n",
    "            if restriction not in Restrictions.permitted.keys():\n",
    "                raise Exception(\"Restriction\", restriction, \"not known\")\n",
    "        \n",
    "        Restrictions.restrictions = restrictions\n",
    "\n",
    "    \n",
    "    def no_inverse_parent(parent):\n",
    "        try: return [Language.symbol_to_idx[parent.inv]]\n",
    "        except: return []\n",
    "\n",
    "\n",
    "    def no_double_exp(parent):\n",
    "        if parent.symbol in ['exp', '^', '^2']:\n",
    "            'Return symbol to idx only of the operators in Language'\n",
    "            return [Language.symbol_to_idx[x] for x in ['exp', '^', '^2'] if x in Language.symbol_to_idx.keys()]\n",
    "        return []\n",
    "\n",
    "    \n",
    "    def no_sqrt_in_log(parent):\n",
    "        if parent.symbol == 'log' and 'sqrt' in Language.symbol_to_idx.keys():\n",
    "            return [Language.symbol_to_idx['sqrt']]\n",
    "        return []\n",
    "\n",
    "    \n",
    "    def no_double_division(parent):\n",
    "        if parent.symbol == '/':\n",
    "            return [Language.symbol_to_idx['/']]\n",
    "        return []\n",
    "\n",
    "    def DistributiveRestriction(not_allowed, parent, n_variables, needed):\n",
    "        not_allowed = list(not_allowed)\n",
    "        if needed + parent.arity > n_variables + Language.use_constants - len(not_allowed):\n",
    "            return not_allowed + [Language.token_to_idx[parent]]\n",
    "        return not_allowed\n",
    "\n",
    "    def no_const_after_unary(parent):\n",
    "        if parent.arity == 1 and Language.use_constants:\n",
    "            return [0]\n",
    "        return []\n",
    "\n",
    "    def no_sqrt_in_sqrt(parent):\n",
    "        if parent.symbol == 'sqrt' and 'sqrt' in Language.symbol_to_idx.keys():\n",
    "            return [Language.symbol_to_idx['sqrt']]\n",
    "        return []\n",
    "\n",
    "    def no_trigo_in_log(parent):\n",
    "        if parent.symbol == 'log':\n",
    "            return [Language.symbol_to_idx[x] for x in ['sin', 'cos', 'tan'] if x in Language.symbol_to_idx.keys()]\n",
    "        return []\n",
    "\n",
    "    def no_trigo_offspring(trigo_offspring):\n",
    "        if trigo_offspring:\n",
    "            return [Language.symbol_to_idx[x] for x in ['sin', 'cos', 'tan'] if x in Language.symbol_to_idx.keys()]\n",
    "        return []\n",
    "\n",
    "    def no_trigo_in_exp(parent):\n",
    "        if parent.symbol == 'exp':\n",
    "            return [Language.symbol_to_idx[x] for x in ['sin', 'cos', 'tan'] if x in Language.symbol_to_idx.keys()]\n",
    "        return []\n",
    "\n",
    "    def no_tan_in_exp(parent):\n",
    "        if parent.symbol == 'exp':\n",
    "            return [Language.symbol_to_idx['tan']]\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def _protected_division(x1, x2):\n",
    "    \"\"\"Closure of division (x1/x2) for zero denominator.\"\"\"\n",
    "    return np.where(np.abs(x2) > 1e-5, np.divide(x1, x2), np.ones(1))\n",
    "\n",
    "def _protected_sqrt(x1):\n",
    "    \"\"\"Closure of square root for negative arguments.\"\"\"\n",
    "    return np.sqrt(np.abs(x1))\n",
    "\n",
    "def _protected_log(x1):\n",
    "    \"\"\"Closure of log for zero and negative arguments.\"\"\"\n",
    "    return np.where(np.abs(x1) > 1e-5, np.log(np.abs(x1)), np.zeros(1))\n",
    "\n",
    "def _protected_exp(x1):\n",
    "    \"\"\"Closure of exp for overflow\"\"\"\n",
    "    return np.where(x1 <= 30, np.exp(x1), np.ones(1)*10686474581524.463)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Language(Restrictions):\n",
    "    function_set_symbols = None\n",
    "    function_set_idx = None\n",
    "    function_set_tokens = None\n",
    "    max_variables = 5\n",
    "    use_constants = None\n",
    "    n_functions = None\n",
    "    size = None\n",
    "\n",
    "    # Dicts so we can change fastly between token, idx and symbol -> idx 0 is reserved for constants and 1 to 10 for variables\n",
    "    symbol_to_idx = {'+': 6, '-': 7, '*': 8, '/': 9, '^2': 10, '^3': 11, '^4': 12,\n",
    "                     'sin': 13, 'cos': 14, 'exp': 15, 'log': 16, 'sqrt': 17, \n",
    "                     'abs': 18, 'max': 19, 'min': 20, 'inv': 21, 'neg': 22,}\n",
    "    idx_to_symbol = None\n",
    "    symbol_to_token = {\n",
    "        '+': Token(function=lambda a, b: a + b, arity=2, symbol='+', inv='-', unprotected_function=lambda a, b: a + b),\n",
    "        '-': Token(function=lambda a, b: a - b, arity=2, symbol='-', inv='+', unprotected_function=lambda a, b: a - b),\n",
    "        '*': Token(function=lambda a, b: a * b, arity=2, symbol='*', inv='/', unprotected_function=lambda a, b: a * b),\n",
    "        '/': Token(function=lambda a, b: _protected_division(a, b), arity=2, symbol='/', inv='*', unprotected_function= lambda a, b: a / b),\n",
    "        '^2': Token(function=lambda a: np.power(a, 2), arity=1, symbol='^2', inv='sqrt', unprotected_function=lambda a: a ** 2),\n",
    "        '^3': Token(function=lambda a: np.power(a, 3), arity=1, symbol='^3', inv='cbrt', unprotected_function=lambda a: a ** 3),\n",
    "        '^4': Token(function=lambda a: np.power(a, 4), arity=1, symbol='^4', inv='sqrt', unprotected_function=lambda a: a ** 4),\n",
    "        'sin': Token(function=lambda a: np.sin(a), arity=1, symbol='sin', unprotected_function=lambda a: sin(a)),\n",
    "        'cos': Token(function=lambda a: np.cos(a), arity=1, symbol='cos', unprotected_function=lambda a: cos(a)),\n",
    "        'exp': Token(function=lambda a: _protected_exp(a), arity=1, symbol='exp', inv='log', unprotected_function=lambda a: exp(a)),\n",
    "        'log': Token(function=lambda a: _protected_log(a), arity=1, symbol='log', inv='exp', unprotected_function=lambda a: log(Abs(a))),\n",
    "        'sqrt': Token(function=lambda a: _protected_sqrt(a), arity=1, symbol='sqrt', inv='^2', unprotected_function=lambda a: sqrt(Abs(a))),\n",
    "        'abs': Token(function=lambda a: np.abs(a), arity=1, symbol='abs', unprotected_function=lambda a: Abs(a)),\n",
    "        'max': Token(function=lambda a, b: np.maximum(a, b), arity=2, symbol='max', unprotected_function=lambda a, b: Max(a, b)),\n",
    "        'min': Token(function=lambda a, b: np.minimum(a, b), arity=2, symbol='min', unprotected_function=lambda a, b: Min(a, b)),\n",
    "        'inv': Token(function=lambda a: np.reciprocal(a), arity=1, symbol='inv', unprotected_function=lambda a: 1 / a),\n",
    "        'neg': Token(function=lambda a: -a, arity=1, symbol='neg', unprotected_function=lambda a: -a),\n",
    "    }\n",
    "    token_to_idx = None\n",
    "    idx_to_token = None\n",
    "\n",
    "    type_1_functions, type_2_functions, type_3_functions, type_4_functions = 0, 0, 0, 0\n",
    "\n",
    "    def __init__(self, function_set_symbols, restrictions, use_constants, max_variables):\n",
    "        super(Language, self).__init__(restrictions)\n",
    "        # Assert that all th function symbols are valid\n",
    "        for function_symbol in function_set_symbols:\n",
    "            if function_symbol not in self.symbol_to_token:\n",
    "                raise ValueError('Unknown function symbol: ' + function_symbol)\n",
    "            if function_symbol in ['+', '-', '*', '/']: Language.type_1_functions += 1\n",
    "            elif function_symbol in ['^2', '^3', '^4', 'exp', 'log', 'sqrt', 'inv', 'neg']: Language.type_2_functions += 1\n",
    "            elif function_symbol in ['sin', 'cos']: Language.type_3_functions += 1\n",
    "            elif function_symbol in ['abs', 'max', 'min']: Language.type_4_functions += 1\n",
    "\n",
    "        # Complete dictionaries\n",
    "        Language.idx_to_symbol = {v: k for k, v in Language.symbol_to_idx.items()}\n",
    "        Language.token_to_idx = {v: Language.symbol_to_idx[k] for k, v in Language.symbol_to_token.items()}\n",
    "        Language.idx_to_token = {v: k for k, v in Language.token_to_idx.items()}\n",
    "\n",
    "        # Functions used in the language\n",
    "        Language.function_set_symbols = function_set_symbols   # List of functions to be used\n",
    "        Language.function_set_idx = [self.symbol_to_idx[x] for x in function_set_symbols]  # List of idx to be used\n",
    "        Language.function_set = [Language.symbol_to_token[function] for function in function_set_symbols]  # List of tokens to be used\n",
    "        Language.use_constants = use_constants   # Boolean to indicate if constants should be used\n",
    "\n",
    "        # Maximum number of variables\n",
    "        Language.n_functions = len(Language.symbol_to_token)\n",
    "        Language.size = Language.max_variables + len(Language.idx_to_symbol) + 1\n",
    "\n",
    "\n",
    "    def get_possibilities(info, n_variables, to_take='all'):\n",
    "        parent = info['function']\n",
    "        distributive = info['distributive']\n",
    "        needed = info['needed']\n",
    "        trigo_offspring = info['trigo_offspring']\n",
    "        arity_one = info['arity_one']\n",
    "        '''\n",
    "        This function returns the list of possible tokens_idx that can be added to the expression tree.\n",
    "        '''\n",
    "        not_allowed = []\n",
    "        for restriction in Restrictions.restrictions:\n",
    "            if restriction == 'DistributiveRestriction':\n",
    "                not_allowed += Restrictions.permitted[restriction](distributive, parent, n_variables, needed)\n",
    "            elif restriction in 'no_trigo_offspring':\n",
    "                not_allowed += Restrictions.permitted[restriction](trigo_offspring)\n",
    "            else:\n",
    "                not_allowed += Restrictions.permitted[restriction](parent)\n",
    "\n",
    "        for token in Language.function_set:\n",
    "            if token.arity == 1:\n",
    "                not_allowed.append(Language.token_to_idx[token])\n",
    "\n",
    "        terminals = [x for x in range(1 - Language.use_constants, n_variables + 1)]\n",
    "        if to_take == 'all':\n",
    "            return list(set(Language.function_set_idx + terminals) - set(not_allowed))\n",
    "        elif to_take == 'terminal':\n",
    "            return list(set(terminals) - set(not_allowed))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Expression():\n",
    "    '''\n",
    "    This class represents an Expression Tree. It is composed by a list of tokens/traversal (ex: [0, 2, 1]) and its one-hot-encoding representation in torch ([[1,0,0],[0,0,1],[0,1,0]]).\n",
    "    '''\n",
    "    def __init__(self, traversal=None, seed=None, max_nodes=15, n_variables=None):\n",
    "        self.max_nodes = max_nodes\n",
    "        self.n_variables = n_variables if n_variables is not None else Language.max_variables\n",
    "        if traversal is None:\n",
    "            self.P = [0 for _ in range(Language.size)]\n",
    "            for i in range(n_variables + 1):\n",
    "                self.P[i] = 0.4/(n_variables + 1) # Probability of choosing a terminal\n",
    "            for i in range(Language.max_variables + 1, Language.size):\n",
    "                symbol = Language.idx_to_symbol[i]\n",
    "                if symbol in Language.function_set_symbols:\n",
    "                    if symbol in ['+', '-', '*', '/']: self.P[i] = 0.3/Language.type_1_functions\n",
    "                    elif symbol in ['^2', '^3', '^4', 'exp', 'log', 'sqrt', 'inv', 'neg']: self.P[i] = 0.22/Language.type_2_functions\n",
    "                    elif symbol in ['sin', 'cos']: self.P[i] = 0.06/Language.type_3_functions\n",
    "                    elif symbol in ['abs', 'max', 'min']: self.P[i] = 0.02/Language.type_4_functions\n",
    "                else:\n",
    "                    self.P[i] = 0\n",
    "            traversal = self.generate_random_expression(seed=seed)\n",
    "        self.traversal = traversal\n",
    "        self.one_hot_encoding = torch.nn.functional.one_hot(torch.Tensor(traversal).to(torch.int64), num_classes=Language.size)\n",
    "        if Language.use_constants:\n",
    "            self.constants = np.abs(np.random.normal(0, 3, size=(self.traversal.count(0),)))\n",
    "\n",
    "\n",
    "    def to_sympy(self):\n",
    "        '''\n",
    "        This function returns the sympy expression of the expression tree.\n",
    "        '''\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10 = symbols('x1, x2, x3, x4, x5, x6, x7, x8, x9, x10')\n",
    "        x = x1, x2, x3, x4, x5, x6, x7, x8, x9, x10\n",
    "        stack = []\n",
    "        const = 0\n",
    "        for idx in self.traversal[::-1]:\n",
    "            if idx > Language.max_variables:\n",
    "                function = Language.idx_to_token[idx]\n",
    "                first_operand = stack.pop()\n",
    "                if function.arity == 1:\n",
    "                    stack.append(function.unprotected_function(first_operand))\n",
    "                else:\n",
    "                    second_operand = stack.pop()\n",
    "                    stack.append(function.unprotected_function(first_operand, second_operand))\n",
    "                \n",
    "            elif idx == 0:\n",
    "                stack.append(self.constants[const])\n",
    "                const += 1\n",
    "            else:\n",
    "                stack.append(x[idx - 1])\n",
    "\n",
    "        return stack[0].simplify()\n",
    "\n",
    "    def evaluate(self, X, constants=None):\n",
    "        '''\n",
    "        This function evaluates the expression tree at a given point x.\n",
    "        '''\n",
    "        if constants is None:\n",
    "            constants = self.constants\n",
    "\n",
    "        stack = []\n",
    "        const = 0\n",
    "        for idx in self.traversal[::-1]:\n",
    "            if idx > Language.max_variables:\n",
    "                function = Language.idx_to_token[idx]\n",
    "                first_operand = stack.pop()\n",
    "                if function.arity == 1:\n",
    "                    stack.append(function.function(first_operand))\n",
    "                else:\n",
    "                    second_operand = stack.pop()\n",
    "                    stack.append(function.function(first_operand, second_operand))\n",
    "                \n",
    "            elif idx == 0:\n",
    "                stack.append(constants[const]*np.ones(X.shape[0]))\n",
    "                const += 1\n",
    "            else:\n",
    "                stack.append(X[:, idx - 1])\n",
    "        \n",
    "        return stack[0]\n",
    "\n",
    "    def loss(self, constants, X, y):\n",
    "        return ((self.evaluate(X, constants) - y)**2).mean()\n",
    "    \n",
    "\n",
    "    def add_node(self, will_be_nodes, arities_stack, function_stack, program, max_nodes, First, P, predefined_choice=None):\n",
    "        if predefined_choice is not None:\n",
    "            choice = predefined_choice\n",
    "        else:\n",
    "            if First:   # If it is the root\n",
    "                possibilities = Language.function_set_idx # Take a function to avoid degenerated expressions\n",
    "            else:\n",
    "                # First let's check what we can add given the context (parent and its not_allowed operands/operators)\n",
    "                possibilities = Language.get_possibilities(info=function_stack[-1], n_variables=self.n_variables)\n",
    "            # Get their correspondent probabilities and normalize them\n",
    "            prob_possibilites = list(map(P.__getitem__, possibilities))\n",
    "            prob_possibilites = [p / sum(prob_possibilites) for p in prob_possibilites]\n",
    "            # Choose a token idx\n",
    "            choice = np.random.choice(possibilities, p=prob_possibilites)\n",
    "\n",
    "        # Determine if we are adding a function or terminal -> Add Function if we got a function and there will be enough nodes to add it and add its children\n",
    "        if  choice > Language.max_variables and will_be_nodes + Language.idx_to_token[choice].arity <= max_nodes:  \n",
    "            program.append(choice)  # Append to program\n",
    "            function = Language.idx_to_token[choice]  # Function to add\n",
    "            arities_stack.append(function.arity)   # Append to arities\n",
    "            will_be_nodes += function.arity # Update the number of nodes that there will be\n",
    "            new_set = set()   # If we are adding a function of the same type, we need to inherit its not_allowed variables if DistributiveRestriction is used\n",
    "            needed = function.arity - 1\n",
    "            trigo_offspring = False if First else function_stack[-1]['trigo_offspring']\n",
    "            arity_one = True\n",
    "            if not First and function.arity == 1 and function_stack[-1]['function'].arity == 1:\n",
    "                arity_one = False\n",
    "            if not First and function_stack[-1]['function'].symbol == Language.idx_to_symbol[choice]:\n",
    "                new_set = function_stack[-1]['distributive']\n",
    "                needed += function_stack[-1]['needed']\n",
    "            if function.symbol in ['sin', 'cos', 'tan', 'tanh']:\n",
    "                trigo_offspring = True\n",
    "            function_stack.append({'function': function, 'trigo_offspring': trigo_offspring, 'distributive': new_set, 'needed': needed, 'arity_one': arity_one})  # Append to function stack\n",
    "        else:\n",
    "            # We need a terminal, add a variable or constant\n",
    "            if choice > Language.max_variables:  # If we got here because of the lack of space, we need to choose a terminal again\n",
    "                # Get the possible terminals and choose one in the same way that it was done before\n",
    "                possibilities = Language.get_possibilities(info=function_stack[-1], n_variables=self.n_variables, to_take='terminal')\n",
    "                prob_possibilites = list(map(P.__getitem__, possibilities)) if not isinstance(P, list) else list(map(P.__getitem__, possibilities))\n",
    "                prob_possibilites = [p / sum(prob_possibilites) for p in prob_possibilites] if prob_possibilites is not None else None\n",
    "                choice = np.random.choice(possibilities, p=prob_possibilites)\n",
    "\n",
    "            program.append(choice)   # Append to program\n",
    "            function_stack[-1]['distributive'].add(choice)  # Add to the not_allowed set\n",
    "            arities_stack[-1] -= 1  # One node added to the parent, so we need to remove one from the arity\n",
    "            while arities_stack[-1] == 0:  # If completed arity of node \n",
    "                arities_stack.pop()        # Remove it from the arity stack\n",
    "                if not arities_stack:      # If there are no more arities, we are done\n",
    "                    break\n",
    "                child_info = function_stack.pop()  # Remove it from the function stack\n",
    "                child_symbol = child_info['function'].symbol\n",
    "                child_set = child_info['distributive']  # Pass info of non-allowed variables to parent if DistributiveRestriction is used\n",
    "                arities_stack[-1] -= 1\n",
    "                if child_symbol == function_stack[-1]['function'].symbol:\n",
    "                    function_stack[-1]['distributive'].update(child_set)\n",
    "            \n",
    "        return will_be_nodes, arities_stack, function_stack, program\n",
    "\n",
    "    def generate_random_expression(self, seed=None):\n",
    "        # Set the seed\n",
    "        np.random.seed(seed)\n",
    "        First = True\n",
    "        max_nodes = np.random.randint(4, self.max_nodes)\n",
    "\n",
    "        will_be_nodes = 1\n",
    "        arities_stack, function_stack, program = [], [], []\n",
    "\n",
    "        while First or arities_stack:   # While there are operands/operators to add\n",
    "            will_be_nodes, arities_stack, function_stack, program = self.add_node(will_be_nodes, arities_stack, function_stack, program, max_nodes, First, self.P)\n",
    "            First = False\n",
    "        return program\n",
    "\n",
    "\n",
    "    def mutate(self, p_random_tree=0.3):\n",
    "        '''\n",
    "        # This function mutates the expression tree.\n",
    "        '''\n",
    "        new_traversal = self.traversal\n",
    "        max_nodes = np.random.randint(4, self.max_nodes)\n",
    "        new_random_tree = np.random.random() < p_random_tree\n",
    "\n",
    "        First = True\n",
    "        will_be_nodes = 1\n",
    "        arities_stack, function_stack, program = [], [], []\n",
    "\n",
    "        prob_mutation = min(2/len(new_traversal), 0.5)\n",
    "        cont = 0\n",
    "\n",
    "        while First or arities_stack:   # While there are operands/operators to add\n",
    "            \n",
    "            # Choose whether to mutate or not\n",
    "            predefined_choice = None if new_random_tree or np.random.rand() < prob_mutation or cont >= len(new_traversal) else new_traversal[cont]\n",
    "\n",
    "            will_be_nodes, arities_stack, function_stack, program = self.add_node(will_be_nodes, arities_stack, function_stack, \n",
    "                                                                                    program, max_nodes, First, self.P, predefined_choice)\n",
    "            First = False\n",
    "            cont += 1\n",
    "\n",
    "        return Expression(program, self.n_variables)\n",
    "\n",
    "    \n",
    "    def optimize_constants(self, X, y):\n",
    "        '''\n",
    "        # This function optimizes the constants of the expression tree.\n",
    "        '''\n",
    "        if 0 not in self.traversal:\n",
    "            return self.traversal\n",
    "        \n",
    "        x0 = np.abs(np.random.normal(0, 1, len(self.constants)))\n",
    "        res = minimize(self.loss, x0, args=(X, y), method='BFGS', options={'disp': False})\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Dataset():\n",
    "\n",
    "    def normalize(tensor, lb=None, up=None):\n",
    "        if lb is None:\n",
    "            lb = tensor.min()\n",
    "            up = tensor.max()\n",
    "            return np.divide(np.subtract(tensor, lb), np.subtract(up, lb)), lb, up\n",
    "        return np.divide(np.subtract(tensor, lb), np.subtract(up, lb))\n",
    "\n",
    "    def __init__(self, n_functions, seed=None):\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        self.n_functions = n_functions\n",
    "        seeds = np.random.randint(0, 1e9, n_functions)\n",
    "        self.dataset = []\n",
    "\n",
    "        for sub_seed in seeds:\n",
    "            row = {}\n",
    "            row['n_obs'] = np.random.randint(10, 100)\n",
    "            row['n_vars'] = np.random.randint(1, Language.max_variables)\n",
    "            row['Expression'] = Expression(seed=sub_seed, n_variables=row['n_vars'])\n",
    "            row['X_lower_bound'] = np.random.uniform(0.05, 6, size=row['n_vars'])\n",
    "            row['X_upper_bound'] = [np.random.uniform(row['X_lower_bound'][i] + 1, 10) for i in range(row['n_vars'])]\n",
    "            row['X'] = np.concatenate([np.random.uniform(row['X_lower_bound'][i], row['X_upper_bound'][i], (row['n_obs'], 1)) for i in range(row['n_vars'])], axis=1)\n",
    "            row['X_norm'] = Dataset.normalize(row['X'], lb=row['X_lower_bound'], up=row['X_upper_bound'])\n",
    "            row['y'] = row['Expression'].evaluate(row['X'])\n",
    "            row['y_norm'], row['y_lower_bound'], row['y_upper_bound'] = Dataset.normalize(row['y'])\n",
    "            self.dataset.append(row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ETIN():\n",
    "    def __init__(self, functions=['+', '*'], restrictions=None, use_constants=True, max_variables=4):\n",
    "        # Create Language of the the Expressions with the given restrictions\n",
    "        Language(functions, restrictions, use_constants, max_variables)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Create an Expression Tree Improver Network (ETIN)\n",
    "        # self.etin_model = ETIN_model(encoded_size=10, emb_size=3, language_size=..., max_seq_size=20)\n",
    "\n",
    "\n",
    "    def train(self, n_functions=1, epochs=10, seed=None):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            dataset = Dataset(n_functions, seed=seed)\n",
    "            dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=self.preprocessing)\n",
    "\n",
    "            for batch, data in enumerate(dataloader):\n",
    "                pass\n",
    "                \n",
    "\n",
    "            return dataset\n",
    "\n",
    "    \n",
    "    def preprocessing(self, data):\n",
    "        input_dataset, input_expression, input_prediction, bounds = [], [], [], []\n",
    "        for row in data:\n",
    "            # Input Dataset\n",
    "            padding = torch.nn.ConstantPad1d((0, Language.max_variables - row['n_vars']), 0)\n",
    "            input_dataset.append(padding(torch.cat([torch.Tensor(row['y_norm']).unsqueeze(1), torch.Tensor(row['X_norm'])], dim=1)))\n",
    "\n",
    "            # Input prediction\n",
    "            new_expression = row['Expression'].mutate()\n",
    "            # Optimize constant values in the expression\n",
    "            new_expression.optimize_constants(row['X'], row['y'])\n",
    "            y = new_expression.evaluate(row['X'])\n",
    "            y_norm = Dataset.normalize(y, lb=row['y_lower_bound'], up=row['y_upper_bound'])\n",
    "            input_prediction.append(padding(torch.cat([torch.Tensor(y_norm).unsqueeze(1), torch.Tensor(row['X_norm'])], dim=1)))\n",
    "\n",
    "            # # Input Expression\n",
    "            # max_length = row['Expression'].max_nodes\n",
    "            # input_expression.append(torch.cat([new_expression.one_hot_encoding, torch.zeros(max_length, Language.size)], dim=0))\n",
    "\n",
    "            # bounds.append((row['X_lower_bound'], row['X_upper_bound'], row['y_lower_bound'], row['y_upper_bound']))\n",
    "\n",
    "        return (input_dataset, input_expression, input_prediction, bounds)\n",
    "\n",
    "\n",
    "\n",
    "etin = ETIN(functions=['+', '-', '*', '/', '^2', '^3', '^4', 'log', 'exp', 'sqrt', 'sin', 'cos'], \n",
    "            restrictions=['no_inverse_parent', 'no_double_exp', 'no_sqrt_in_log', 'no_double_division', 'DistributiveRestriction', \n",
    "                          'no_const_after_unary', 'no_sqrt_in_sqrt', 'no_trigo_in_log', 'no_trigo_offspring'], \n",
    "            use_constants=True, \n",
    "            max_variables=4)\n",
    "dataset = etin.train(n_functions=100, seed=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfd94461c018b7b186f60b1189aa3ffb3c547d36d49166324bfa07b535f359c9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
